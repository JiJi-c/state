experiment:
  name: "vci_pretrain_${loss.name}_${model.nhead}_${model.nlayers}"
  local: "local"
  compiled: false
  profile:
    enable_profiler: false
    profile_steps: [10, 100]
    max_steps: 110 # This is used only when profile is enabled
  num_epochs: 16
  num_nodes: 1
  num_gpus_per_node: 1
  master: localhost
  port: 12356
  val_check_interval: 250 # Number of steps between tests
  limit_val_batches: 0.005 # Use 1% of validation dataset
  checkpoint:
    path: /scratch/ctc/ML/vci/checkpoint/pretrain
    save_top_k: 4
    monitor: trainer/train_loss
    every_n_train_steps: 1000

wandb:
  project: "vci"

embeddings:
  esm2:
    checkpoint: /scratch/ctc/ML/uce/all_species_pe_tokens.torch
    embedding_file: /large_storage/ctc/ML/data/cell/misc/Homo_sapiens.GRCh38.gene_symbol_to_embedding_ESM2.pt
    # TODO: Eliminate the need for these attributes. This informtation can be
    # read from the above file.
    cnt: 145469
    size: 5120

validations:
  diff_exp:
    enable: true
    eval_interval_multiple: 10
    obs_pert_col: gene
    obs_filter_label: non-targeting
    top_k_rank: 200
    method: null
    dataset: /large_storage/ctc/datasets/vci/validation/rpe1_top5000_variable.h5ad
    dataset_name: rpe1_top5000_variable
  perturbation:
    enable: true
    eval_interval_multiple: 2
    pert_col: gene
    ctrl_label: non-targeting
    dataset: /large_storage/ctc/datasets/vci/validation/replogle_perturbation.h5ad #use this path for deaware
    dataset_name: replogle_perturbation

dataset:
  name: "vci"
  seed: 42
  train: /scratch/ctc/ML/uce/h5ad_train_dataset.csv
  val: /scratch/ctc/ML/uce/h5ad_val_dataset.csv
  data_dir: /large_experiments/goodarzilab/mohsen/cellxgene/processed
  protein_emb_file_format: /large_storage/ctc/datasets/vci/training/gene_embidx_mapping.torch
  pad_length: 2048
  pad_token_idx: 0
  cls_token_idx: 3
  chrom_token_right_idx: 2
  P: 512
  N: 512
  num_cells:  36238464 # TODO: Is this required

tokenizer:
  token_dim: 5120

model:
  name: 'vci'
  batch_size: 64
  emsize: 256
  d_hid: 1024
  nhead: 16
  nlayers: 8
  dropout: 0.1
  output_dim: 256 # TODO: Is emsize different from this?
  use_flash_attention: true

optimizer:
  max_lr: 4e-4
  weight_decay: 0.01
  start: 1e-4
  end: 1.0
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1

loss:
  name: "mmd" # Available options: cross_entropy, wasserstein, kl_divergence
  normalization: False
