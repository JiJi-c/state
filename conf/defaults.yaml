experiment:
  name: "run_1"
  local: "local"
  compiled: false
  num_epochs: 16
  num_nodes: 1
  num_gpus_per_node: 4
  master: localhost
  port: 12355
  val_check_interval: 1001 # Number of steps between tests
  limit_val_batches: 0.01 # Use 1% of validation dataset
  checkpoint:
    path: "/scratch/ctc/vci/checkpoint"
    save_top_k: 4
    monitor: train_loss

embeddings:
  esm2:
    checkpoint: "/large_storage/ctc/public/dataset/vci/all_species_pe_tokens.torch"
    # TODO: Eliminate the need for these attributes. This informtation can be
    # read from the above file.
    cnt: 145469
    size: 5120

dataset:
  name: "vci_h5ad"
  train: "/large_storage/ctc/public/dataset/vci/h5ad_train_dataset.csv"
  test: "/large_storage/ctc/public/dataset/vci/h5ad_test_dataset.csv"
  data_dir: "/large_experiments/goodarzilab/mohsen/cellxgene/processed"
  protein_emb_file_format: "/large_storage/ctc/public/dataset/vci/gene_embidx_mapping.torch"
  pad_length: 2048
  pad_token_idx: 0
  cls_token_idx: 3
  chrom_token_right_idx: 2
  P: 512
  N: 512
  num_cells:  36238464 # TODO: Is this required

tokenizer:
  token_dim: 5120

model:
  name: 'vci'
  batch_size: 32
  emsize: 256
  d_hid: 1024
  nhead: 8
  nlayers: 4
  dropout: 0.1
  output_dim: 256 # TODO: Is emsize different from this?

optimizer:
  max_lr: 4e-4
  weight_decay: 0.01
  start: 1e-4
  end: 1.0
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
