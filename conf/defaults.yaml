experiment:
  name: "run_1"
  local: "local"
  compiled: false
  num_epochs: 16
  num_nodes: 1
  num_gpus_per_node: 2
  master: localhost
  port: 12355
  val_check_interval: 1000 # Number of steps between tests
  limit_val_batches: 0.01 # Use 1% of validation dataset
  checkpoint:
    path: "/scratch/ctc/ML/vci/checkpoint"
    save_top_k: 4
    monitor: train_loss
    every_n_train_steps: 1000

wandb:
  project: "vci"

embeddings:
  esm2:
    checkpoint: "/scratch/ctc/ML/uce/all_species_pe_tokens.torch"
    # TODO: Eliminate the need for these attributes. This informtation can be
    # read from the above file.
    cnt: 145469
    size: 5120

dataset:
  name: "vci"
  train: "/scratch/ctc/ML/vci/h5ad_recount_train.csv"
  test: "/scratch/ctc/ML/vci/h5ad_recount_test.csv"
  data_dir: "/large_storage/ctc/ML/data/cell/recount/czi_processed"
  protein_emb_file_format: "/scratch/ctc/ML/uce/model_files/gene_embidx_mapping.torch"
  pad_length: 2048
  pad_token_idx: 0
  cls_token_idx: 3
  chrom_token_right_idx: 2
  P: 512
  N: 512
  num_cells:  36238464 # TODO: Is this required

tokenizer:
  token_dim: 5120

model:
  name: 'vci'
  batch_size: 64
  emsize: 256
  d_hid: 1024
  nhead: 16
  nlayers: 8
  dropout: 0.1
  output_dim: 256 # TODO: Is emsize different from this?

optimizer:
  max_lr: 4e-4
  weight_decay: 0.01
  start: 1e-4
  end: 1.0
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
